{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d696e5f-b48e-464c-8282-f0e08b27da9b",
   "metadata": {},
   "source": [
    "# Welcome to my own Lab on Web Scrapping!\n",
    "The goal is to understand what is webscrapping, how to use it and why it is so useful for data scientist. This lab is divided in different parts let's start and have fun!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d73acad-9135-435f-a4dc-50dcb82930a6",
   "metadata": {},
   "source": [
    "## Useful Librairies for Web Scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f189fce-0777-44dd-a5e1-46c0ebaba36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c9389c-670f-4ef8-994b-e7ba4d515d40",
   "metadata": {},
   "source": [
    "## Section 1: How to load a web page with request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bad5090-7655-4142-a789-743e193dc344",
   "metadata": {},
   "source": [
    "Request allows you to get useful inforamtion on the webpage you are interested in. More specifically it allows you to send HTTP requests using Python (HTTP requests are communication protocol beyond the scope of this class but still very interesting). \r\n",
    "The HTTP request returns a Response Object with all the response data (content, encoding, status, etc ).There are several methods you can use to see if the web u is correc, w the page sour and morec. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b82a909-8cfc-45c6-96f6-57eb6e6f03e2",
   "metadata": {},
   "source": [
    "Below are some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e702b925-a064-427c-bb5e-00c6d5687857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "403\n",
      "<html>\n",
      "<head><title>403 Forbidden</title></head>\n",
      "<body>\n",
      "<center><h1>403 Forbidden</h1></center>\n",
      "<hr><center>nginx</center>\n",
      "</body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "url1 = \"https://earlham.edu\"\n",
    "response = requests.get(url)\n",
    "print(response.status_code)  # Check if the request was successful\n",
    "print(response.text)  # View the page source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52859fdc-560e-4b43-966d-710d619c75c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "<!doctype html>\n",
      "<html>\n",
      "<head>\n",
      "    <title>Example Domain</title>\n",
      "\n",
      "    <meta charset=\"utf-8\" />\n",
      "    <meta http-equiv=\"Content-type\" content=\"text/html; charset=utf-8\" />\n",
      "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\" />\n",
      "    <style type=\"text/css\">\n",
      "    body {\n",
      "        background-color: #f0f0f2;\n",
      "        margin: 0;\n",
      "        padding: 0;\n",
      "        font-family: -apple-system, system-ui, BlinkMacSystemFont, \"Segoe UI\", \"Open Sans\", \"Helvetica Neue\", Helvetica, Arial, sans-serif;\n",
      "        \n",
      "    }\n",
      "    div {\n",
      "        width: 600px;\n",
      "        margin: 5em auto;\n",
      "        padding: 2em;\n",
      "        background-color: #fdfdff;\n",
      "        border-radius: 0.5em;\n",
      "        box-shadow: 2px 3px 7px 2px rgba(0,0,0,0.02);\n",
      "    }\n",
      "    a:link, a:visited {\n",
      "        color: #38488f;\n",
      "        text-decoration: none;\n",
      "    }\n",
      "    @media (max-width: 700px) {\n",
      "        div {\n",
      "            margin: 0 auto;\n",
      "            width: auto;\n",
      "        }\n",
      "    }\n",
      "    </style>    \n",
      "</head>\n",
      "\n",
      "<body>\n",
      "<div>\n",
      "    <h1>Example Domain</h1>\n",
      "    <p>This domain is for use in illustrative examples in documents. You may use this\n",
      "    domain in literature without prior coordination or asking for permission.</p>\n",
      "    <p><a href=\"https://www.iana.org/domains/example\">More information...</a></p>\n",
      "</div>\n",
      "</body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "url2 = \"https://example.com\"\n",
    "response = requests.get(url2)\n",
    "print(response.status_code)  # Check if the request was successful\n",
    "print(response.text)  # View the page source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d53d2e-5d77-4d78-be59-f9a3ef08d37d",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "Explore some of the methods you can use on the response object. Find the encoding type of the web page and the time between the request was sent and the response received. You can find more information here https://www.geeksforgeeks.org/python-requests-tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9f28de-fe1f-416d-908a-d7abfffe9ba1",
   "metadata": {},
   "source": [
    "Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa90fdcd-dc51-4049-a9ac-5fe8ec146842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0:00:00.087845\n",
      "Encoding: UTF-8\n"
     ]
    }
   ],
   "source": [
    "print(f\"Time: {response.elapsed}\")\n",
    "print(f\"Encoding: {response.encoding}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b13cec5-d06d-4a1b-9054-41a1983cb441",
   "metadata": {},
   "source": [
    "# Section 2: Explore HTML basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860bb0ff-23ce-410a-acc3-0715fff9c46c",
   "metadata": {},
   "source": [
    "It is crucial to understand the **HTML basics** when performing web scraping. The base of any web page is made with HTML code; therefore, understanding HTML is essential for effective web scraping. A web page has an HTML structure, and we use key tags such as `head`, `title`, and `body` to indicate the format and information we want to extract from the page. \r\n",
    "\r\n",
    "Below are some useful tags and their descriptions:\r\n",
    "\r\n",
    "- **`<!DOCTYPE html>`**: Declares that the document is an HTML5 document.\r\n",
    "- **`<html>`**: The root element of an HTML page.\r\n",
    "- **`<head>`**: Contains meta information about the HTML page.\r\n",
    "- **`<title>`**: Specifies a title for the HTML page (shown in the browser's title bar or the page's tab).\r\n",
    "- **`<body>`**: Defines the document's body and contains all the visible content, such as headings, paragraphs, and images.\r\n",
    "- **`<h1>`**: Defines a large heading.\r\n",
    "- **`<p>`**: Defines a paragraph.\r\n",
    "agraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca34258-155b-4fd0-9222-6334b7b188bf",
   "metadata": {},
   "source": [
    "Here is an example of a simple webpage. From the first part of the lab we can see I use a href attribute to indicate a hyperlink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d108f313-4b9a-408b-893b-2a7d46b0f0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = \"\"\"\n",
    "<html>\n",
    "    <head><title>Vassily</title></head>\n",
    "    <body>\n",
    "        <h1>Hello, World!</h1>\n",
    "        <a href=\"https://example.com\">Visit Example</a>\n",
    "    </body>\n",
    "</html>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1be3f22-bbe5-417d-8eb8-11ba4e7b610f",
   "metadata": {},
   "source": [
    "I can use BeautifulSoup to parse info from this page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a530eaba-3c50-4c31-abf9-4679b974ff3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example\n",
      "Hello, World!\n",
      "https://example.com\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "print(soup.title.text)  # Output: Example\n",
    "print(soup.h1.text)     # Output: Hello, World!\n",
    "print(soup.a['href'])  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d77256-ef5d-4cb3-be35-c3aa2b45a0d2",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "Make your own html page on something you like, add hyper links and use Beautiful soup to parse info."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3b99f2-782b-4ec4-a59c-9edb42a274e3",
   "metadata": {},
   "source": [
    "Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6a7892f1-1dfc-4cc9-b343-4d8c977926fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_web_page = \"\"\"\n",
    "<html>\n",
    "    <head><title>Golf</title></head>\n",
    "    <body>\n",
    "        <h1>What is golf?</h1>\n",
    "        <p>The modern game of golf originated in 15th century Scotland. The 18-hole round was created at the Old Course at St Andrews in 1764. Golf's first major, and the world's oldest golf tournament, is The Open Championship, also known as The Open, which was first played in 1860 at the Prestwick Golf Club in Ayrshire, Scotland. This is one of the four major championships in men's professional golf, the other three being played in the United States: The Masters, the U.S. Open, and the PGA Championship. Below is more information about golf !</p>\n",
    "        <a href=\"https://https://en.wikipedia.org/wiki/Golf\">More info on golf</a>\n",
    "    </body>\n",
    "</html>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "03fc246e-d59b-4c65-ba23-8850c0e72655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Golf\n",
      "What is golf?\n",
      "https://https://en.wikipedia.org/wiki/Golf\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(my_web_page, 'html.parser')\n",
    "print(soup.title.text)  # Output: Example\n",
    "print(soup.h1.text)     # Output: Hello, World!\n",
    "print(soup.a['href'])  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5981e97-35b6-40ec-bc37-cabb1913d933",
   "metadata": {},
   "source": [
    "Now that you know how html pages work it is time to use these skills on real webpage and start collecting data!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5abf4a3-04f9-4dbb-8613-3ac523ba4a51",
   "metadata": {},
   "source": [
    "# Section 3: Collect data from real webpage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0797d24-6e39-4719-8275-c663d8e302be",
   "metadata": {},
   "source": [
    "After some practice we are now able to understand the structure of an html page. With that in mind, we can collect data on any webpage without even having to look at it. We can automate the collection of data from webpage only using their html structure. Let's say we want to collect the news titles of an online magazine. We can automate the process with the skills we developed so far."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6f7eb3-3aca-49db-9143-4045dc82dd38",
   "metadata": {},
   "source": [
    "For instance, I know to know the news title of cnn, we can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d8b570cc-c088-40cd-9097-cecb9bb917e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: \n",
      "\n",
      "\n",
      "\n",
      " (https://www.cnn.com)\n",
      "2: \n",
      "                  \n",
      "                  US\n",
      "                 (https://www.cnn.com/us)\n",
      "3: \n",
      "                  \n",
      "                  World\n",
      "                 (https://www.cnn.com/world)\n",
      "4: \n",
      "                  \n",
      "                  Politics\n",
      "                 (https://www.cnn.com/politics)\n",
      "5: \n",
      "                  \n",
      "                  Business\n",
      "                 (https://www.cnn.com/business)\n",
      "6: \n",
      "                  \n",
      "                  Health\n",
      "                 (https://www.cnn.com/health)\n",
      "7: \n",
      "                  \n",
      "                  Entertainment\n",
      "                 (https://www.cnn.com/entertainment)\n",
      "8: \n",
      "                  \n",
      "                  Style\n",
      "                 (https://www.cnn.com/style)\n",
      "9: \n",
      "                  \n",
      "                  Travel\n",
      "                 (https://www.cnn.com/travel)\n",
      "10: \n",
      "                  \n",
      "                  Sports\n",
      "                 (https://www.cnn.com/sports)\n",
      "11: \n",
      "                  \n",
      "                  Underscored\n",
      "                 (https://www.cnn.com/cnn-underscored)\n",
      "12: \n",
      "                  \n",
      "                  Science\n",
      "                 (https://www.cnn.com/science)\n",
      "13: \n",
      "                  \n",
      "                  Climate\n",
      "                 (https://www.cnn.com/climate)\n",
      "14: \n",
      "                  \n",
      "                  Weather\n",
      "                 (https://www.cnn.com/weather)\n",
      "15: \n",
      "                  \n",
      "                  Ukraine-Russia War\n",
      "                 (https://www.cnn.com/world/europe/ukraine)\n",
      "16: \n",
      "                  \n",
      "                  Israel-Hamas War\n",
      "                 (https://www.cnn.com/world/middleeast/israel)\n",
      "17: \n",
      "                  \n",
      "                  Black Friday Deals\n",
      "                 (https://www.cnn.com/cnn-underscored/deals/black-friday)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'href'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m titles \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, title \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(titles, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtitle\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtitle[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/mounts/pollock/anaconda/lib/python3.11/site-packages/bs4/element.py:1573\u001b[0m, in \u001b[0;36mTag.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1570\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m   1571\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"tag[key] returns the value of the 'key' attribute for the Tag,\u001b[39;00m\n\u001b[1;32m   1572\u001b[0m \u001b[38;5;124;03m    and throws an exception if it's not there.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1573\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrs[key]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'href'"
     ]
    }
   ],
   "source": [
    "url = \"https://cnn.com/\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find all article titles and their links\n",
    "titles = soup.find_all('a')\n",
    "for i, title in enumerate(titles, start=1):\n",
    "    print(f\"{i}: {title.text} ({title['href']})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53a6d1b-2fd8-4617-bae4-ad0590b8cc25",
   "metadata": {},
   "source": [
    "### Exercise 3:\n",
    "It is also possible to collect other info on the content of the page. For example it is possible to collect data on tables in the webpages, forms, list and images. In the following exercise get info on possible tables and images on the same website"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754ce444-5c0c-4d1c-87d7-a72d09c91170",
   "metadata": {},
   "source": [
    "Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4890c1b2-9eac-4d93-9db7-e66445da698c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: None \n",
      "2: None \n",
      "3: None \n",
      "4: None \n",
      "5: None \n",
      "6: None \n",
      "7: None \n",
      "8: None \n",
      "9: None \n",
      "10: None \n",
      "11: None \n",
      "12: None \n",
      "13: None \n",
      "14: None \n",
      "15: None \n",
      "16: None \n",
      "17: None \n",
      "18: None \n",
      "19: None \n",
      "20: None \n",
      "21: None \n",
      "22: None \n",
      "23: None \n",
      "24: None \n",
      "25: None \n",
      "26: None \n",
      "27: None \n",
      "28: None \n",
      "29: None \n",
      "30: None \n",
      "31: None \n",
      "32: None \n",
      "33: None \n",
      "34: None \n",
      "35: None \n",
      "36: None \n",
      "37: None \n",
      "38: None \n",
      "39: None \n",
      "40: None \n",
      "41: None \n",
      "42: None \n",
      "43: None \n",
      "44: None \n",
      "45: None \n",
      "46: None \n",
      "47: None \n",
      "48: None \n",
      "49: None \n",
      "50: None \n",
      "51: None \n",
      "52: None \n",
      "53: None \n",
      "54: None \n",
      "55: None \n"
     ]
    }
   ],
   "source": [
    "# There is no alternative text describing the images. But it is still useful to identify how many images are on the website.\n",
    "images = soup.find_all('img')\n",
    "for i, image in enumerate(images, start=1):\n",
    "    print(f\"{i}: {image.alt} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d44376da-e9f6-4854-a902-c83ac2f8d4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: None \n",
      "2: None \n",
      "3: None \n",
      "4: None \n",
      "5: None \n",
      "6: None \n",
      "7: None \n",
      "8: None \n",
      "9: None \n",
      "10: None \n",
      "11: None \n",
      "12: None \n",
      "13: None \n",
      "14: None \n",
      "15: None \n",
      "16: None \n",
      "17: None \n",
      "18: None \n",
      "19: None \n",
      "20: None \n",
      "21: None \n",
      "22: None \n",
      "23: None \n"
     ]
    }
   ],
   "source": [
    "# Again the meta data associated to this website is hidden. We can still get info on how many element have meta data.\n",
    "# The lack of info might be due to data protection by cnn. Other public and less protected website might be easier to get info from.\n",
    "\n",
    "images = soup.find_all('meta')\n",
    "for i, image in enumerate(images, start=1):\n",
    "    print(f\"{i}: {image.keywords} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13b65ba-9787-4e53-ae79-86601f0612fd",
   "metadata": {},
   "source": [
    "# The End!\n",
    "Thank you for participating and investing time in this lab, I hope you enjoyed it!\n",
    "- By Vassily for DS 401"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8d69a0-b5e3-46b1-9844-acc55aab8bd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
